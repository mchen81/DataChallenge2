{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef085ab",
   "metadata": {},
   "source": [
    "# Part2. Choose the best Model\n",
    "\n",
    "In the part 1, we've decided the most suitable features. Here I will use those features to train different models, and find out the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b3e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('cash or e-zpass train.csv', low_memory=False)\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df.drop('Date', axis=1)\n",
    "df['Vehicle Class'] = labelencoder.fit_transform(df['Vehicle Class'])\n",
    "df['Entrance'] = labelencoder.fit_transform(df['Entrance'])\n",
    "df['Exit'] = labelencoder.fit_transform(df['Exit'])\n",
    "df['Payment Type (Cash or E-ZPass)'] = labelencoder.fit_transform(df['Payment Type (Cash or E-ZPass)'])\n",
    "\n",
    "X = df[['Interval Beginning Time', 'Vehicle Class', 'Entrance', 'Exit', 'Vehicle Count']]\n",
    "Y = df['Payment Type (Cash or E-ZPass)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd461f4",
   "metadata": {},
   "source": [
    "# Sampling Data\n",
    "\n",
    "Even though this dataset contains more than 6 millions rows, the time of training decision tree model is quite fast(less than 1 minute). However, for the boosting algorithms like random forest and gradient boosting, they take hours to finish the training. In order to speed up the compairison, it'd be better to reduce the amount of rows. Here I only take 8% of 6-million rows. And the boosting algorithms still need to take about 5 minutes to complete the training process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00b4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(frac =.08)\n",
    "sample_X = sample_df[['Interval Beginning Time', 'Vehicle Class', 'Entrance', 'Exit', 'Vehicle Count']]\n",
    "sample_Y = sample_df['Payment Type (Cash or E-ZPass)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081979b",
   "metadata": {},
   "source": [
    "# Classification Algorithms\n",
    "\n",
    "Again, this is a classification problem, so we I picked up four appropriate algorithms, wich are Decision tree, Random Forest, Gradient Boosting and Logistic Regression. And I am going to show the comparison of these four with default parameters and choose the best model in terms of speed and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b4659",
   "metadata": {},
   "source": [
    "## Candidate 1: Decision Tree\n",
    "\n",
    "In the part1, we've seen the model trained by decision tree can gain about 73% accuracy. But for now, let's forget it and see how its performance is on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c345cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6852437198469472"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(sample_X, sample_Y, test_size=0.2)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "hyp = clf.predict(test_x)\n",
    "accuracy_score(test_y, hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd5364",
   "metadata": {},
   "source": [
    "## Candidate 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22a8a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7134108301447346"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rfc_clf = RandomForestClassifier()\n",
    "rfc_clf.fit(train_x, train_y)\n",
    "hyp = rfc_clf.predict(test_x)\n",
    "accuracy_score(test_y, hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcab60",
   "metadata": {},
   "source": [
    "## Candidate 3: Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fd6087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6825923307270005"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(train_x, train_y)\n",
    "hyp = lr_clf.predict(test_x)\n",
    "accuracy_score(test_y, hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53910ea5",
   "metadata": {},
   "source": [
    "## Candidate 4: Logistic Regreesion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072274dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894443520212943"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc_clf = GradientBoostingClassifier()\n",
    "gbc_clf.fit(train_x, train_y)\n",
    "hyp = gbc_clf.predict(test_x)\n",
    "accuracy_score(test_y, hyp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470535f",
   "metadata": {},
   "source": [
    "# Model Decision\n",
    "\n",
    "As the cells showing above, the random forest gets the best result in predicting sampled data. However, I still not going to train a model by the random forest because I've tried a model with it and I waited for more than 2 hours but the the traning was never end. Therefore, I will choose a model from Logistic Regresion and Decision Tree, because both of them got a efficient and similar result.\n",
    "\n",
    "# Model Tuning \n",
    "\n",
    "To determine which model I am going to use in the final prediction, I would like to apply the grid search and find out the best parameters for both LR and DT. And because both algorithms have remarkable speed, I decided to expand the sample size to gain a more reliable comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70573aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisiton tree best best_parameters: {'criterion': 'gini', 'max_depth': 21}\n",
      "Decistion tree best score: 0.7485118452254621\n"
     ]
    }
   ],
   "source": [
    "# Note: this process takes about one hour\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parametes = {'criterion':['gini', 'entropy'],\n",
    "             'max_depth':[16,17,18,19,20,21,22]}\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), parametes, cv=5)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print('Decisiton tree best best_parameters:', clf.best_params_)\n",
    "print('Decistion tree best score:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd5fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best_parameters: {'C': 1e-10, 'penalty': 'l2'}\n",
      "Logistic Regression score: 0.6856422602220273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "grid_values = {'penalty': ['l1','l2'], 'C': [1e-10, 1e-09, 1e-08, 1e-07,1]}\n",
    "clf = GridSearchCV(LogisticRegression(solver='liblinear'), grid_values, cv=5)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print('Logistic Regression best_parameters:', clf.best_params_)\n",
    "print('Logistic Regression score:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f38575",
   "metadata": {},
   "source": [
    "# Final Decision\n",
    "\n",
    "From the results showing above, the accuracy of LR sticks with aroung 68%. On the other hand, decision tree with gini and depth of 21 has the best accuracy. Therefore, I will use this set of hyper parameters in our final prediction.  \n",
    "\n",
    "To ensure the result, let's train two models, the first one is default decision tree without hyper parameters. The second one is tuned decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1843085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7341294333526869"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "default_clf = DecisionTreeClassifier()\n",
    "default_clf.fit(train_x, train_y)\n",
    "hyp = default_clf.predict(test_x)\n",
    "accuracy_score(test_y, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10cb8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7485347622763048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_clf = DecisionTreeClassifier(criterion='gini', max_depth=21)\n",
    "tuned_clf.fit(train_x, train_y)\n",
    "hyp = tuned_clf.predict(test_x)\n",
    "accuracy_score(test_y, hyp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
